---
title: "Notice du démonstrateur"
output:
  pdf_document:
    fig_width: 7
    fig_height: 6
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ce démonstrateur comporte 5 pages. Pour chacunes d'entre elles, vous trouverez les explications ci-dessous.

## Comment utiliser ce démonstrateur

Page d'accueil. Cliquez sur le bouton de téléchargement pour arriver sur cette notice.

## Qu'est ce qu'un SVM?

Si vous n'avez pas de notion en terme de machine learning, nous vous expliquons ici brievement la méthode de machine à vecteur de support (SVM) .Cette courte introduction est inspiré du cours de Christophe Hurlin, que vous pouvez retrouver en intégralité en cliquant le lien ci-dessous:  
<https://sites.google.com/view/christophe-hurlin/teaching-resources/support-vector-machine>.

## Présentation de nos données

Vous trouverez ici toutes les informations sur les données que nous avons utilisées pour construire ce démonstrateur. Vous pouvez retrouver la base de données sur le site de kaggle, en cliquant sur le lien ci dessous:   
<https://www.kaggle.com/mlg-ulb/creditcardfraud>. 

De plus, nous vous expliquons les indices de performances que nous utilisons dans notre analyse.

## Intéraction avec le SVM 


```{r, out.width = "100%", include=TRUE, echo=FALSE}
library(knitr)
include_graphics("C:/Users/mikew/OneDrive/Documents/GitHub/buvat_langevin_walter/www/capture.PNG")
```


Ici, nous montrons les matrices de confusions sur l'échantillon d'apprentissage et sur l'échantillon test (les observations bien classées sont en bleu, les autres en oranges), la sensitivité, la spécificité et le taux d'erreur qui leurs sont associées, ainsi que la courbe ROC obtenu par la méthode du SVM.  

Cette page est intéractive. Ainsi, vous pouvez choisir:  
- **La fonction noyau** qui permettra de rendre l'échantillon linéairement séparable (linéaire, polynomial, radial ou sigmoïde).  
- **Le degré du polynôme** que l'on peut fixer à 3, 4 ou 5 que si nous avons choisi  le kernel polynomial.    
- **Le coût de pénalisation**, que l'on peut fixer à 1,3,5 ou 10. Pour rappel, plus ce dernier est important, plus on fait d'erreur, plus il est grand, plus on risque de faire de l'overfeeting. 

## Comparaison du SVM avec les autres méthdes de machines learning

Cet onglet se décompose en quatre parties distinctes:  

### Recherche du meilleur SVM

Page d'introduction de l'onglet. On explique brievement la comparaison des méthodes.

### Régression logistique

On compare les résulats du SVM à ceux de la régression logistique.

### Arbre de régression

On compare les résultat du SVM à ceux de l'arbre de classification.

### Méthode KNN

On compare les résultat du SVM à ceux de la méthode des plus proches vosins.  Dans cette page, vous pourrez modifier le nombre de voisins qui seront utilisés lors de l'estimation de la méthode KNN (allant de 1 à 20 voisins).  

## Remerciements

Un dernier onglet où nous vous donnons le lien de notre page Github. Sur cette page internet, vous trouverez l'ensemble des codes et des ressources que nous avons utilisé pour la création de ce démonstrateur.  
Nous citons aussi les personnes qui nous ont été d'une grande importance lors de la création de ce projet.

